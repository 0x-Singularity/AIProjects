{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0cL5DhDwHLJvvom5PaZSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0x-Singularity/AIProjects/blob/main/ConvolutionalNN_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Convolutional Neural Network to recognize the Fashion MNIST dataset"
      ],
      "metadata": {
        "id": "65K4jy8XvBo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the data**"
      ],
      "metadata": {
        "id": "AcddbwiG5v6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist"
      ],
      "metadata": {
        "id": "iQfxFaK-vLBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing tensorflow and the fashion MNIST dataset we will call the data.load_data() function to laod the data set. Since we're calling an image data set you may think we're loading in images, but we're actually loading in 60,000 28x28 2D arrays that are populated with the pixel values of the grayscale images. The function returns two tuples. The first tuple contains the following: training_images, training_lables and the second tuple contains the: test_images, test_labels"
      ],
      "metadata": {
        "id": "4VkPQT8CvWSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNIO4XPov5EZ",
        "outputId": "31ce7c36-d7c8-487b-91ef-eada7bb651a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shaping our data**\n",
        "\n",
        "Since this is a convolutional neural network it needs the data delivered in a specific way. The number of channels must be included since CNNs work with multi-dimensional data. We're using Grayscale images which have 1 channel, as opposed to RGB images which have 3 channels. The reshape function changes the shape of training_images and test_images to include this channel dimension. After reshaping the training and test images will have a shape of (60000, 28, 28,1) meaning 60000 images that are 28x28 and have a channel of 1 (Greyscale)\n",
        "\n",
        "The pixels in the image are then normalized. The pixel values originally were ranged from 0 to 255, but after being normalized (divided by 255) it gives us a float that ranges from 0.0 to 1.0. The purpose of this is because neural networks typically perform better when the input values are in a smaller, more normalized range, as it helps the model converge faster during training."
      ],
      "metadata": {
        "id": "GJQzpF3GwlTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_images = training_images.reshape(60000, 28, 28, 1)\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "v2bwEkfr0sa7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building our Model**\n",
        "\n",
        "This is where we get to define our Convolution Neural Network. The model begins with our 2D convolutional layer where 64 filters of size 3x3 are applied to the input images, which have an input shape of 28x28(pixels),1(channel, Greyscale). The ReLU activation function is used to introduce non-linearity.\n",
        "\n",
        "After that we have a 2x2 max pooling layer which is applied todownsample the feature maps by reducing their spatial dimensions while keeping important features. Another convolutional layer with 64 filters of size 3x3 is added, again using the ReLU activation function and 2x2 max pooling. Why is it done twice? This is done to progressively extract more complex features and reduce the spatial dimensions of the image, making it easier for the network to learn important patterns hwile reducing the computational complexity.\n",
        "\n",
        "Next, the feature maps are flattened into a 1D vector, which allows the data to be passed into fully connected layers. A dense layer with 128 neaurs is added, using ReLU activation to help the model learn complex patterns. Finally, the output layer consists of 10 neurons (one for each class in the MNIST dataset), and uses the softmax activation function to output the probabilities for each class."
      ],
      "metadata": {
        "id": "qaRAwVwS1DO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu', input_shape = (28,28,1)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC3hVpfx1kGU",
        "outputId": "91bc0dcb-0f2e-49f8-e382-3f92af1ddbda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training our Model and the results**\n",
        "\n",
        "The model is first compield using the Adam optimizer, which is an optimization algorithm that adjusts the learning rate dynamically during training to improve performance. The loss function 'spare_categorical_crossentropy' is chosen, and the metric we want to track is accuracy.\n",
        "\n",
        "The model.fit() function is used to train the model on the training dataset. The function takes in the training_images and training_labels as input and runs the training for 50 epochs.\n",
        "\n",
        "After the training the model.evaluate() function is used to test the model on the unseen test dataset. This evaluates the models performance by calculating its accuracy and loss on the test data.\n",
        "\n",
        "Finally, model.predict() is used to generate predictions for the test images. This returns an array of probabilities for each class, indicating how likely the model thinks each image belongs to a particular category."
      ],
      "metadata": {
        "id": "Un0YLJZN4GyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jelitp64HEW",
        "outputId": "fe0dd041-06d4-4e6d-96ba-f409a7c8d530"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 51ms/step - accuracy: 0.7814 - loss: 0.6013\n",
            "Epoch 2/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 50ms/step - accuracy: 0.8866 - loss: 0.3055\n",
            "Epoch 3/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 49ms/step - accuracy: 0.9082 - loss: 0.2499\n",
            "Epoch 4/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 50ms/step - accuracy: 0.9181 - loss: 0.2185\n",
            "Epoch 5/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 49ms/step - accuracy: 0.9334 - loss: 0.1838\n",
            "Epoch 6/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 49ms/step - accuracy: 0.9381 - loss: 0.1634\n",
            "Epoch 7/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 50ms/step - accuracy: 0.9480 - loss: 0.1386\n",
            "Epoch 8/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 52ms/step - accuracy: 0.9527 - loss: 0.1244\n",
            "Epoch 9/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 50ms/step - accuracy: 0.9606 - loss: 0.1064\n",
            "Epoch 10/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 52ms/step - accuracy: 0.9660 - loss: 0.0908\n",
            "Epoch 11/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 51ms/step - accuracy: 0.9719 - loss: 0.0768\n",
            "Epoch 12/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 51ms/step - accuracy: 0.9734 - loss: 0.0708\n",
            "Epoch 13/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 49ms/step - accuracy: 0.9771 - loss: 0.0603\n",
            "Epoch 14/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 50ms/step - accuracy: 0.9812 - loss: 0.0513\n",
            "Epoch 15/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 51ms/step - accuracy: 0.9836 - loss: 0.0454\n",
            "Epoch 16/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 50ms/step - accuracy: 0.9846 - loss: 0.0424\n",
            "Epoch 17/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 50ms/step - accuracy: 0.9869 - loss: 0.0357\n",
            "Epoch 18/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 50ms/step - accuracy: 0.9849 - loss: 0.0388\n",
            "Epoch 19/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 49ms/step - accuracy: 0.9891 - loss: 0.0296\n",
            "Epoch 20/20\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 50ms/step - accuracy: 0.9875 - loss: 0.0321\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9148 - loss: 0.5346\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step\n",
            "[2.71017921e-19 2.50428041e-18 1.04402758e-17 2.77200280e-21\n",
            " 9.10928964e-27 1.09046906e-13 2.78340693e-19 1.34073828e-15\n",
            " 2.16821625e-26 9.99999940e-01]\n",
            "9\n"
          ]
        }
      ]
    }
  ]
}